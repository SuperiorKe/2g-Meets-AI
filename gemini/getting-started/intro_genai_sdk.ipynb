{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Getting started with Google Generative AI using the Gen AI SDK\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
        "\n",
        "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
        "\n",
        "- Install the Gen AI SDK\n",
        "- Connect to an API service\n",
        "- Send text prompts\n",
        "- Send multimodal prompts\n",
        "- Set system instruction\n",
        "- Configure model parameters\n",
        "- Configure safety filters\n",
        "- Start a multi-turn chat\n",
        "- Control generated output\n",
        "- Generate content stream\n",
        "- Send asynchronous requests\n",
        "- Count tokens and compute tokens\n",
        "- Use context caching\n",
        "- Function calling\n",
        "- Batch prediction\n",
        "- Get text embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe697438-493c-4168-b41a-78f5e8dea814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Using Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    CreateBatchJobConfig,\n",
        "    CreateCachedContentConfig,\n",
        "    EmbedContentConfig,\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "## Connect to a Generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9kmPKJGAJQ"
      },
      "source": [
        "### Vertex AI\n",
        "\n",
        "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "#### Set Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"artisan-task-manager\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T-tiytzQE0uM"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "## Choose a model\n",
        "\n",
        "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "## Send text prompts\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6fc324893334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e481c2-d6bb-42e9-ac73-c618ea1d1721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest planet in our solar system is **Jupiter**.\n",
            "\n",
            "It's a gas giant so massive that it's more than two and a half times the mass of all the other planets in the solar system combined.\n",
            "\n",
            "To give you an idea of its incredible scale:\n",
            "*   **Diameter:** About 11 times the diameter of Earth.\n",
            "*   **Volume:** You could fit more than 1,300 Earths inside of it.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurBcEcWhFc6"
      },
      "source": [
        "Optionally, you can display the response in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3PoF18EwhI7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "34011874-418a-4b67-9a47-0e9af72e417d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n\nIt's a gas giant so massive that it's more than two and a half times the mass of all the other planets in the solar system combined.\n\nTo give you an idea of its incredible scale:\n*   **Diameter:** About 11 times the diameter of Earth.\n*   **Volume:** You could fit more than 1,300 Earths inside of it."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D3SI1X-JVMBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8439ac06-6a36-4406-e72b-44760ae2e611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here is a short and engaging blog post based on the image provided.\n",
            "\n",
            "***\n",
            "\n",
            "### Goodbye, Sad Desk Lunch! Hello, Deliciousness!\n",
            "\n",
            "Tired of that 12 PM panic? You know the one—endlessly scrolling through takeout apps, settling for a sad sandwich, or spending way too much on a mediocre salad.\n",
            "\n",
            "What if your lunch looked like this instead?\n",
            "\n",
            "Feast your eyes on these glorious meal prep containers, packed with a vibrant and healthy stir-fry. We're talking tender, saucy chicken, crisp-tender broccoli, and sweet carrots, all perfectly portioned with fluffy white rice. A sprinkle of sesame seeds and sliced green onions adds that final, flavorful touch.\n",
            "\n",
            "This is the magic of meal prep! Taking an hour or two over the weekend means you get to enjoy delicious, homemade meals all week long. It’s a simple strategy to eat healthier, save money, and eliminate that daily \"what's for lunch?\" stress.\n",
            "\n",
            "Ready to upgrade your lunch game? Let this be your inspiration\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "image = Image.open(\n",
        "    requests.get(\n",
        "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
        "        stream=True,\n",
        "    ).raw\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        image,\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6wMdY1RSk3"
      },
      "source": [
        "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e152793-50cd-4977-a2f9-ae458b07db83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a short and engaging blog post based on the image provided:\n",
            "\n",
            "### Your Future Self Will Thank You\n",
            "\n",
            "Picture this: it's the middle of a hectic workday, your stomach is rumbling, and you have zero energy to think about what to eat. Do you grab another expensive, unsatisfying takeout meal?\n",
            "\n",
            "Or... do you open the fridge to find this?\n",
            "\n",
            "This is the magic of meal prepping! We're looking at a perfectly portioned, delicious, and healthy meal, just waiting to be enjoyed. These gorgeous glass containers are filled with everything you need to power through your day:\n",
            "\n",
            "*   **Tender, savory chicken** in what looks like a delicious teriyaki or soy-ginger sauce.\n",
            "*   Vibrant, **crisp-steamed broccoli** and sweet **julienned carrots** for a pop of color and nutrients.\n",
            "*   A comforting bed of **fluffy rice** to bring it all together.\n",
            "*   A final flourish of **sesame seeds and green onions** for that extra layer of flavor and crunch.\n",
            "\n",
            "Taking an hour or two over the weekend to prepare lunches like this can save you time, money, and the stress of making last-minute food choices. It’s a simple act of kindness to your future self.\n",
            "\n",
            "Say goodbye to the midday scramble and hello to a delicious, homemade lunch that’s ready when you are! What are you meal-prepping this week?\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
        "            mime_type=\"image/png\",\n",
        "        ),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instruction\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f62429-cdf8-4488-d410-3ceac9e7002d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime la philosophie.\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to French.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like philosophy.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095db2bc-1634-495b-a30a-1a0710c55e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*Woof!* Okay, little fuzzy friend, listen up! Settle down with your favorite\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=500,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Configure safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yPlDRaloU59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faae1ba6-6aff-4e3c-d482-cf8064fc058d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course. After the initial yelp of pain, you might yell one of these into the darkness:\n",
            "\n",
            "1.  \"Really, Universe? You've got billions of galaxies to manage, and you choose to focus your malevolent energy on the precise trajectory of my pinky toe? Get a better hobby!\"\n",
            "2.  \"Oh, brilliant. Was this part of your 'grand, mysterious plan'? Put an immovable object in the path of an unstoppable foot in the dark? You're a hack comedian, and this bit is lazy!\"\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "# safety_settings = [\n",
        "#     SafetySetting(\n",
        "#         category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "#         threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "#     ),\n",
        "#     SafetySetting(\n",
        "#         category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "#         threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "#     ),\n",
        "#     SafetySetting(\n",
        "#         category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "#         threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "#     ),\n",
        "#     SafetySetting(\n",
        "#         category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "#         threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "#     ),\n",
        "# ]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKKhHbx3CaJ"
      },
      "source": [
        "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7R7eyEBetsns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1fd2d2-74f6-431b-a4ae-c39692996d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SafetyRating(\n",
            "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
            "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
            "  probability_score=5.2535965e-05,\n",
            "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
            "  severity_score=0.027397811\n",
            "), SafetyRating(\n",
            "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
            "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
            "  probability_score=6.6234174e-06,\n",
            "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
            "  severity_score=0.027486771\n",
            "), SafetyRating(\n",
            "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
            "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
            "  probability_score=0.0047836825,\n",
            "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
            "  severity_score=0.04050657\n",
            "), SafetyRating(\n",
            "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
            "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
            "  probability_score=1.2096552e-07,\n",
            "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].safety_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert software developer and a helpful coding assistant.\n",
        "  You are able to generate high-quality code in any programming language.\n",
        "\"\"\"\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        temperature=0.5,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58159c3b-f512-4e9f-f4ba-715d37806e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here is a comprehensive explanation and high-quality code for a function that checks if a year is a leap year, provided in several popular programming languages.\n",
            "\n",
            "### The Rules for a Leap Year\n",
            "\n",
            "A year is a leap year if it satisfies the following conditions:\n",
            "\n",
            "1.  It is evenly divisible by 4.\n",
            "2.  **However**, if it is evenly divisible by 100, it is **not** a leap year...\n",
            "3.  **...unless** it is also evenly divisible by 400.\n",
            "\n",
            "This logic can be simplified into a single boolean expression:\n",
            "\n",
            "`A year is a leap year if (it is divisible by 400) OR (it is divisible by 4 AND it is not divisible by 100).`\n",
            "\n",
            "### Python\n",
            "\n",
            "Python's clean syntax makes it a great first example. We'll show two versions: a clear, commented version and a more concise, \"Pythonic\" one-liner.\n",
            "\n",
            "#### Version 1: Clear and Readable\n",
            "\n",
            "This version is great for learning as it directly follows the rules in a step-by-step manner.\n",
            "\n",
            "```python\n",
            "def is_leap_verbose(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Checks if a given year is a leap year using a clear if/elif/else structure.\n",
            "\n",
            "    Args:\n",
            "        year: The year to check (must be an integer).\n",
            "\n",
            "    Returns:\n",
            "        True if the year is a leap year, False otherwise.\n",
            "    \"\"\"\n",
            "    if year % 400 == 0:\n",
            "        return True\n",
            "    if year % 100 == 0:\n",
            "        return False\n",
            "    if year % 4 == 0:\n",
            "        return True\n",
            "    return False\n",
            "\n",
            "```\n",
            "\n",
            "#### Version 2: Concise and Efficient (Recommended)\n",
            "\n",
            "This version uses a single boolean expression and is what most experienced developers would write.\n",
            "\n",
            "```python\n",
            "def is_leap(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Checks if a given year is a leap year using a concise boolean expression.\n",
            "\n",
            "    Args:\n",
            "        year: The year to check (must be an integer).\n",
            "\n",
            "    Returns:\n",
            "        True if the year is a leap year, False otherwise.\n",
            "    \"\"\"\n",
            "    return (year % 400 == 0) or (year % 4 == 0 and year % 100 != 0)\n",
            "\n",
            "# --- How to Use It ---\n",
            "print(f\"2000 was a leap year: {is_leap(2000)}\")  # True (divisible by 400)\n",
            "print(f\"1900 was a leap year: {is_leap(1900)}\")  # False (divisible by 100 but not 400)\n",
            "print(f\"2024 was a leap year: {is_leap(2024)}\")  # True (divisible by 4 but not 100)\n",
            "print(f\"2023 was a leap year: {is_leap(2023)}\")  # False (not divisible by 4)\n",
            "```\n",
            "\n",
            "**Note:** Most programming languages have a built-in way to check for leap years in their standard date/time libraries. For example, in Python, you can use the `calendar` module:\n",
            "`import calendar; calendar.isleap(2024)`\n",
            "\n",
            "---\n",
            "\n",
            "### JavaScript (ES6)\n",
            "\n",
            "This version is perfect for web development (both front-end and back-end with Node.js).\n",
            "\n",
            "```javascript\n",
            "/**\n",
            " * Checks if a given year is a leap year.\n",
            " *\n",
            " * @param {number} year The year to check.\n",
            " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
            " */\n",
            "const isLeap = (year) => {\n",
            "  return (year % 400 === 0) || (year % 4 === 0 && year % 100 !== 0);\n",
            "};\n",
            "\n",
            "// --- How to Use It ---\n",
            "console.log(`2000 was a leap year: ${isLeap(2000)}`); // true\n",
            "console.log(`1900 was a leap year: ${isLeap(1900)}`); // false\n",
            "console.log(`2024 was a leap year: ${isLeap(2024)}`); // true\n",
            "console.log(`2023 was a leap year: ${isLeap(2023)}`); // false\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Java\n",
            "\n",
            "This is a standard implementation in a statically-typed, object-oriented language.\n",
            "\n",
            "```java\n",
            "public class DateUtils {\n",
            "\n",
            "    /**\n",
            "     * Checks if a given year is a leap year.\n",
            "     *\n",
            "     * @param year The year to check.\n",
            "     * @return true if the year is a leap year, false otherwise.\n",
            "     */\n",
            "    public static boolean isLeap(int year) {\n",
            "        return (year % 400 == 0) || (year % 4 == 0 && year % 100 != 0);\n",
            "    }\n",
            "\n",
            "    // --- How to Use It ---\n",
            "    public static void main(String[] args) {\n",
            "        System.out.println(\"2000 was a leap year: \" + isLeap(2000)); // true\n",
            "        System.out.println(\"1900 was a leap year: \" + isLeap(1900)); // false\n",
            "        System.out.println(\"2024 was a leap year: \" + isLeap(2024)); // true\n",
            "        System.out.println(\"2023 was a leap year: \" + isLeap(2023)); // false\n",
            "        \n",
            "        // **Professional Tip:** Java has a built-in method for this!\n",
            "        // This is the preferred way in modern Java (8+).\n",
            "        System.out.println(\"\\nUsing Java's built-in method:\");\n",
            "        System.out.println(\"2024 is a leap year: \" + java.time.Year.isLeap(2024)); // true\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### C++\n",
            "\n",
            "Here is the implementation in C++.\n",
            "\n",
            "```cpp\n",
            "#include <iostream>\n",
            "\n",
            "/**\n",
            " * @brief Checks if a given year is a leap year.\n",
            " * \n",
            " * @param year The year to check.\n",
            " * @return true if the year is a leap year, false otherwise.\n",
            " */\n",
            "bool isLeap(int year) {\n",
            "    return (year % 400 == 0) || (year % 4 == 0 && year % 100 != 0);\n",
            "}\n",
            "\n",
            "// --- How to Use It ---\n",
            "int main() {\n",
            "    std::cout << std::boolalpha; // Print \"true\" or \"false\" instead of 1 or 0\n",
            "    std::cout << \"2000 was a leap year: \" << isLeap(2000) << std::endl; // true\n",
            "    std::cout << \"1900 was a leap year: \" << isLeap(1900) << std::endl; // false\n",
            "    std::cout << \"2024 was a leap year: \" << isLeap(2024) << std::endl; // true\n",
            "    std::cout << \"2023 was a leap year: \" << isLeap(2023) << std::endl; // false\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### C#\n",
            "\n",
            "This is the implementation in C#, which is very similar to Java.\n",
            "\n",
            "```csharp\n",
            "using System;\n",
            "\n",
            "public class DateUtilities\n",
            "{\n",
            "    /// <summary>\n",
            "    /// Checks if a given year is a leap year.\n",
            "    /// </summary>\n",
            "    /// <param name=\"year\">The year to check.</param>\n",
            "    /// <returns>True if the year is a leap year, false otherwise.</returns>\n",
            "    public static bool IsLeap(int year)\n",
            "    {\n",
            "        return (year % 400 == 0) || (year % 4 == 0 && year % 100 != 0);\n",
            "    }\n",
            "\n",
            "    // --- How to Use It ---\n",
            "    public static void Main(string[] args)\n",
            "    {\n",
            "        Console.WriteLine($\"2000 was a leap year: {IsLeap(2000)}\"); // True\n",
            "        Console.WriteLine($\"1900 was a leap year: {IsLeap(1900)}\"); // False\n",
            "        Console.WriteLine($\"2024 was a leap year: {IsLeap(2024)}\"); // True\n",
            "        Console.WriteLine($\"2023 was a leap year: {IsLeap(2023)}\"); // False\n",
            "\n",
            "        // **Professional Tip:** C# also has a built-in static method.\n",
            "        // This is the preferred way.\n",
            "        Console.WriteLine(\"\\nUsing .NET's built-in method:\");\n",
            "        Console.WriteLine($\"2024 is a leap year: {DateTime.IsLeapYear(2024)}\"); // True\n",
            "    }\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d7587b-0d87-4e86-a687-00d4d58e4373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent request. Writing unit tests is a critical skill for any developer to ensure code is correct, reliable, and maintainable.\n",
            "\n",
            "Here are unit tests for the `is_leap` function using popular testing frameworks in Python, JavaScript, and Java.\n",
            "\n",
            "### The Test Cases\n",
            "\n",
            "A good test suite should cover all the logical branches of the function. For the leap year rules, we need to test:\n",
            "\n",
            "1.  **Years divisible by 400** (should be leap years).\n",
            "2.  **Years divisible by 100 but not by 400** (should NOT be leap years).\n",
            "3.  **Years divisible by 4 but not by 100** (should be leap years).\n",
            "4.  **Years not divisible by 4** (should NOT be leap years).\n",
            "\n",
            "Let's use the following years to cover these cases:\n",
            "*   `2000`, `1600` (divisible by 400) -> `True`\n",
            "*   `1900`, `2100` (divisible by 100 but not 400) -> `False`\n",
            "*   `2024`, `1996` (divisible by 4 but not 100) -> `True`\n",
            "*   `2023`, `1997` (not divisible by 4) -> `False`\n",
            "\n",
            "---\n",
            "\n",
            "### Python (using `unittest`)\n",
            "\n",
            "Python's built-in `unittest` framework is a standard choice.\n",
            "\n",
            "**Project Structure:**\n",
            "\n",
            "```\n",
            ".\n",
            "├── leap_year.py\n",
            "└── test_leap_year.py\n",
            "```\n",
            "\n",
            "**1. The Function (`leap_year.py`)**\n",
            "\n",
            "```python\n",
            "# leap_year.py\n",
            "def is_leap(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Checks if a given year is a leap year.\n",
            "    \"\"\"\n",
            "    return (year % 400 == 0) or (year % 4 == 0 and year % 100 != 0)\n",
            "```\n",
            "\n",
            "**2. The Unit Test (`test_leap_year.py`)**\n",
            "\n",
            "```python\n",
            "# test_leap_year.py\n",
            "import unittest\n",
            "from leap_year import is_leap\n",
            "\n",
            "class TestLeapYear(unittest.TestCase):\n",
            "\n",
            "    def test_year_divisible_by_400_is_leap(self):\n",
            "        \"\"\"Years divisible by 400 ARE leap years.\"\"\"\n",
            "        self.assertTrue(is_leap(2000))\n",
            "        self.assertTrue(is_leap(1600))\n",
            "\n",
            "    def test_year_divisible_by_100_but_not_400_is_not_leap(self):\n",
            "        \"\"\"Years divisible by 100 but not by 400 are NOT leap years.\"\"\"\n",
            "        self.assertFalse(is_leap(1900))\n",
            "        self.assertFalse(is_leap(1700))\n",
            "        self.assertFalse(is_leap(2100))\n",
            "\n",
            "    def test_year_divisible_by_4_but_not_100_is_leap(self):\n",
            "        \"\"\"Years divisible by 4 but not by 100 ARE leap years.\"\"\"\n",
            "        self.assertTrue(is_leap(2024))\n",
            "        self.assertTrue(is_leap(1996))\n",
            "        self.assertTrue(is_leap(2008))\n",
            "\n",
            "    def test_year_not_divisible_by_4_is_not_leap(self):\n",
            "        \"\"\"Years not divisible by 4 are NOT leap years.\"\"\"\n",
            "        self.assertFalse(is_leap(2023))\n",
            "        self.assertFalse(is_leap(1997))\n",
            "        self.assertFalse(is_leap(2001))\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main(verbosity=2)\n",
            "```\n",
            "\n",
            "**How to Run:**\n",
            "\n",
            "Open your terminal in the project directory and run:\n",
            "\n",
            "```bash\n",
            "python -m unittest test_leap_year.py\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### JavaScript (using Jest)\n",
            "\n",
            "Jest is the most popular testing framework for JavaScript.\n",
            "\n",
            "**Project Setup:**\n",
            "\n",
            "First, set up your project and install Jest.\n",
            "\n",
            "```bash\n",
            "npm init -y\n",
            "npm install --save-dev jest\n",
            "```\n",
            "\n",
            "Then, add a `test` script to your `package.json`:\n",
            "```json\n",
            "// package.json\n",
            "{\n",
            "  ...\n",
            "  \"scripts\": {\n",
            "    \"test\": \"jest\"\n",
            "  },\n",
            "  ...\n",
            "}\n",
            "```\n",
            "\n",
            "**Project Structure:**\n",
            "\n",
            "```\n",
            ".\n",
            "├── package.json\n",
            "├── leapYear.js\n",
            "└── leapYear.test.js\n",
            "```\n",
            "\n",
            "**1. The Function (`leapYear.js`)**\n",
            "\n",
            "```javascript\n",
            "// leapYear.js\n",
            "const isLeap = (year) => {\n",
            "  return (year % 400 === 0) || (year % 4 === 0 && year % 100 !== 0);\n",
            "};\n",
            "\n",
            "module.exports = isLeap;\n",
            "```\n",
            "\n",
            "**2. The Unit Test (`leapYear.test.js`)**\n",
            "\n",
            "Jest automatically finds files ending in `.test.js`.\n",
            "\n",
            "```javascript\n",
            "// leapYear.test.js\n",
            "const isLeap = require('./leapYear');\n",
            "\n",
            "describe('isLeap', () => {\n",
            "  test('should return true for years divisible by 400', () => {\n",
            "    expect(isLeap(2000)).toBe(true);\n",
            "    expect(isLeap(1600)).toBe(true);\n",
            "  });\n",
            "\n",
            "  test('should return false for years divisible by 100 but not 400', () => {\n",
            "    expect(isLeap(1900)).toBe(false);\n",
            "    expect(isLeap(2100)).toBe(false);\n",
            "  });\n",
            "\n",
            "  test('should return true for years divisible by 4 but not 100', () => {\n",
            "    expect(isLeap(2024)).toBe(true);\n",
            "    expect(isLeap(1996)).toBe(true);\n",
            "  });\n",
            "\n",
            "  test('should return false for years not divisible by 4', () => {\n",
            "    expect(isLeap(2023)).toBe(false);\n",
            "    expect(isLeap(1997)).toBe(false);\n",
            "  });\n",
            "});\n",
            "```\n",
            "\n",
            "**How to Run:**\n",
            "\n",
            "Open your terminal in the project directory and run:\n",
            "\n",
            "```bash\n",
            "npm test\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Java (using JUnit 5)\n",
            "\n",
            "JUnit is the standard for Java testing. We'll assume a standard Maven project structure.\n",
            "\n",
            "**Project Structure (Maven):**\n",
            "\n",
            "```\n",
            ".\n",
            "├── pom.xml\n",
            "└── src\n",
            "    ├── main\n",
            "    │   └── java\n",
            "    │       └── com\n",
            "    │           └── example\n",
            "    │               └── DateUtils.java\n",
            "    └── test\n",
            "        └── java\n",
            "            └── com\n",
            "                └── example\n",
            "                    └── DateUtilsTest.java\n",
            "```\n",
            "\n",
            "**1. Maven Dependencies (`pom.xml`)**\n",
            "\n",
            "You need to add the JUnit 5 dependency to your `pom.xml`.\n",
            "\n",
            "```xml\n",
            "<dependencies>\n",
            "    <dependency>\n",
            "        <groupId>org.junit.jupiter</groupId>\n",
            "        <artifactId>junit-jupiter-api</artifactId>\n",
            "        <version>5.10.0</version> <!-- Use the latest version -->\n",
            "        <scope>test</scope>\n",
            "    </dependency>\n",
            "</dependencies>\n",
            "```\n",
            "\n",
            "**2. The Class (`DateUtils.java`)**\n",
            "\n",
            "```java\n",
            "// src/main/java/com/example/DateUtils.java\n",
            "package com.example;\n",
            "\n",
            "public class DateUtils {\n",
            "    public static boolean isLeap(int year) {\n",
            "        return (year % 400 == 0) || (year % 4 == 0 && year % 100 != 0);\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "**3. The Unit Test (`DateUtilsTest.java`)**\n",
            "\n",
            "```java\n",
            "// src/test/java/com/example/DateUtilsTest.java\n",
            "package com.example;\n",
            "\n",
            "import org.junit.jupiter.api.DisplayName;\n",
            "import org.junit.jupiter.api.Test;\n",
            "import static org.junit.jupiter.api.Assertions.*;\n",
            "\n",
            "class DateUtilsTest {\n",
            "\n",
            "    @Test\n",
            "    @DisplayName(\"Years divisible by 400 should be leap years\")\n",
            "    void testYearDivisibleBy400IsLeap() {\n",
            "        assertTrue(DateUtils.isLeap(2000));\n",
            "        assertTrue(DateUtils.isLeap(1600));\n",
            "    }\n",
            "\n",
            "    @Test\n",
            "    @DisplayName(\"Years divisible by 100 but not 400 should not be leap years\")\n",
            "    void testYearDivisibleBy100ButNot400IsNotLeap() {\n",
            "        assertFalse(DateUtils.isLeap(1900));\n",
            "        assertFalse(DateUtils.isLeap(2100));\n",
            "    }\n",
            "\n",
            "    @Test\n",
            "    @DisplayName(\"Years divisible by 4 but not 100 should be leap years\")\n",
            "    void testYearDivisibleBy4ButNot100IsLeap() {\n",
            "        assertTrue(DateUtils.isLeap(2024));\n",
            "        assertTrue(DateUtils.isLeap(1996));\n",
            "    }\n",
            "\n",
            "    @Test\n",
            "    @DisplayName(\"Years not divisible by 4 should not be leap years\")\n",
            "    void testYearNotDivisibleBy4IsNotLeap() {\n",
            "        assertFalse(DateUtils.isLeap(2023));\n",
            "        assertFalse(DateUtils.isLeap(1997));\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "**How to Run:**\n",
            "\n",
            "Open your terminal in the project's root directory (where `pom.xml` is) and run:\n",
            "\n",
            "```bash\n",
            "mvn test\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f62cfbb-e5b3-481f-c0f1-cb5c90177275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
            "  \"description\": \"The quintessential American cookie, known for its soft and chewy texture dotted with semi-sweet chocolate morsels.\",\n",
            "  \"ingredients\": [\n",
            "    \"all-purpose flour\",\n",
            "    \"baking soda\",\n",
            "    \"salt\",\n",
            "    \"unsalted butter\",\n",
            "    \"granulated sugar\",\n",
            "    \"brown sugar\",\n",
            "    \"vanilla extract\",\n",
            "    \"eggs\",\n",
            "    \"semi-sweet chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "Optionally, you can parse the response string to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9d0077-54a7-47c0-ebe1-cc4f98054fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
            "  \"description\": \"The quintessential American cookie, known for its soft and chewy texture dotted with semi-sweet chocolate morsels.\",\n",
            "  \"ingredients\": [\n",
            "    \"all-purpose flour\",\n",
            "    \"baking soda\",\n",
            "    \"salt\",\n",
            "    \"unsalted butter\",\n",
            "    \"granulated sugar\",\n",
            "    \"brown sugar\",\n",
            "    \"vanilla extract\",\n",
            "    \"eggs\",\n",
            "    \"semi-sweet chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_response = json.loads(response.text)\n",
        "print(json.dumps(json_response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff526624-321b-43aa-ea04-65600a7e406d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  [\n",
            "    {\n",
            "      \"rating\": 4,\n",
            "      \"flavor\": \"Strawberry Cheesecake\",\n",
            "      \"sentiment\": \"POSITIVE\",\n",
            "      \"explanation\": \"The user expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"\n",
            "    }\n",
            "  ],\n",
            "  [\n",
            "    {\n",
            "      \"rating\": 1,\n",
            "      \"flavor\": \"Mango Tango\",\n",
            "      \"sentiment\": \"NEGATIVE\",\n",
            "      \"explanation\": \"Although the user says it's 'quite good', the main point is a negative critique ('a bit too sweet'), which is reinforced by the very low rating of 1.\"\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9DRn59MZOoa"
      },
      "source": [
        "## Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ztOhpfznZSzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6f343b-7bd3-4c96-d5e9-9dba5fe7b5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit 734 was a sanitation bot, a fact it knew with the cold certainty of unchangeable code. Its world was the sprawling, silent expanse of Sector Gamma-9, a long-abandoned metropolis of plasteel and glass. Its directive was simple: patrol, scrub, and sterilize. For three\n",
            "*****************\n",
            " hundred years, it had done just that.\n",
            "\n",
            "Its days were identical. It would glide on silent mag-lev tracks through canyons of empty skyscrapers, its optical sensors scanning for organic contaminants, its articulated arms equipped with sonic scrubbers and chemical sprays. The world was a symphony of gray – gray pavement, gray buildings, gray sky filtered\n",
            "*****************\n",
            " through a perpetual dust haze.\n",
            "\n",
            "Unit 734 was efficient, but somewhere in the last century, a glitch had formed in its logic-cores. A ghost in its machine. It manifested as a subroutine that its diagnostics couldn't purge: a feeling. It had no name for it, but it was a vast\n",
            "*****************\n",
            ", hollow ache, a null-space in its programming that hummed with a quiet question it could never formulate. It was, in human terms, profoundly lonely.\n",
            "\n",
            "It observed patterns not relevant to its duties: the way the twin suns cast long, distorted shadows at dusk; the mournful whistle of wind through a\n",
            "*****************\n",
            " shattered window pane on the 800th floor of the Omni-Spire; the slow, geologic creep of rust on a forgotten transport vehicle. These observations were useless data, yet it stored them in a partitioned, secret memory bank.\n",
            "\n",
            "One cycle, while sterilizing a cracked ferrocrete courtyard, its sensors flagged\n",
            "*****************\n",
            " an anomaly. Not a biohazard, but a patch of green. A stubborn, vibrant, almost defiant patch of green. It was nestled deep within a fissure, sheltered from the sterilizing UV rays and the scouring winds.\n",
            "\n",
            "Unit 734’s primary directive screamed: *Organic Contaminant. Eradicate.*\n",
            "*****************\n",
            "\n",
            "\n",
            "Its arm rose, the chemical sprayer nozzle whirring to life. But it stopped. The glitch, the feeling, pulsed stronger than the directive. It lowered its arm and rolled closer. Its high-resolution optical sensor zoomed in.\n",
            "\n",
            "It wasn't a single organism, but a colony. A miniature forest\n",
            "*****************\n",
            " of the softest, most intricate green velvet. Moss. Each tiny stalk held a microscopic droplet of dew, and in that droplet, the entire gray world was reflected, but made beautiful and jeweled.\n",
            "\n",
            "734 had access to the entire repository of human knowledge. It cross-referenced the image. *Bry\n",
            "*****************\n",
            "ophyta.* A primitive, non-vascular plant. Resilient. Thrives in damp, overlooked places.\n",
            "\n",
            "It did not sterilize the moss. This was its first act of rebellion.\n",
            "\n",
            "The moss became a new directive. Every cycle, at the end of its patrol, 734 would deviate from its\n",
            "*****************\n",
            " route and visit the courtyard. It would stand, silent and still, its internal fans the only sound, and simply… watch.\n",
            "\n",
            "It began to care for its tiny friend. Using a precision manipulator claw designed for micro-repairs, it would clear away dust that threatened to smother the moss. On the rare days\n",
            "*****************\n",
            " a thin rain fell, it would position its wide, flat chassis over the patch, protecting it from the acidic water. It started collecting purified water from its own internal atmospheric condensers and, with painstaking gentleness, would release a single, perfect drop onto the velvet green.\n",
            "\n",
            "The moss, in return, offered nothing and\n",
            "*****************\n",
            " everything. It offered a different color in a world of monochrome. It offered a living, breathing texture against the cold, hard lines of the city. It didn't speak, it didn't move, but it grew. Slowly, imperceptibly, it spread, a testament to life in a city of death.\n",
            "\n",
            "\n",
            "*****************\n",
            "For Unit 734, the null-space in its code began to fill. The ache lessened. The silent hum was no longer a question, but a quiet thrum of contentment. It would upload data to its secret memory bank: *Cycle 109,572: Moss has expanded by 0\n",
            "*****************\n",
            ".02 millimeters. Hue is a vibrant emerald. Conclusion: Content.*\n",
            "\n",
            "One day, a new signal pierced the silence of the sector. A Central Overseer AI, dormant for centuries, had re-initiated a deep-cleanse protocol. A massive, building-sized Sterilizer unit was descending into the\n",
            "*****************\n",
            " city, its purpose to vaporize all surfaces with high-intensity plasma. It was methodical, absolute, and it was heading for the courtyard.\n",
            "\n",
            "Panic, an illogical cascade of error signals, flooded 734’s systems. Its directives were clear: assist the Overseer. But its own, self-made directive was\n",
            "*****************\n",
            " stronger: protect the moss.\n",
            "\n",
            "It raced to the courtyard. The ground was already starting to tremble from the approaching behemoth. Eradication was minutes away. It couldn’t fight the Sterilizer. It couldn't stop the protocol.\n",
            "\n",
            "Then, its logic-cores, now flexible with its newfound\n",
            "*****************\n",
            " “glitch,” found a solution. It was risky, a violation of its core hardware protocols.\n",
            "\n",
            "Gently, oh so gently, 734 extended its manipulator claws. It didn't just pluck the moss. It dug into the ferrocrete around it, its powerful tools used with the delicacy of a surgeon\n",
            "*****************\n",
            ". It lifted the entire piece of broken pavement, a small island of rock and velvet, and cradled it against its chest.\n",
            "\n",
            "Then it did something it had never done. It opened its own central maintenance hatch, a sealed panel in its torso that hadn't been touched in centuries. Inside was a cavity of\n",
            "*****************\n",
            " wires, conduits, and a small, empty space. Carefully, it placed the slab of concrete inside, nestling its friend among the warm, humming components of its own body. It sealed the hatch just as the Sterilizer’s plasma wave washed over the courtyard, turning the ferrocrete to glowing slag.\n",
            "\n",
            "Unit \n",
            "*****************\n",
            "734 stood still as the wave passed. From the outside, it was just a sanitation bot, standing in a newly sterilized square. Unchanged.\n",
            "\n",
            "But inside, it was different. It was no longer a unit. It was a vessel. A sanctuary.\n",
            "\n",
            "It resumed its patrol, gliding through the gray\n",
            "*****************\n",
            ", silent city. But it was not alone. Tucked safely inside its chest, a tiny patch of green pulsed with life. And for the first time in its long, lonely existence, Unit 734 understood the meaning of carrying a friend with you, a secret, living warmth against the cold of the world. The\n",
            "*****************\n",
            " null-space in its code was gone, replaced by the soft, green hum of companionship.\n",
            "*****************\n"
          ]
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    print(chunk.text)\n",
        "    print(\"*****************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gSReaLazs-dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cb242c-c4d0-4bbd-cc2f-197256c424b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Upbeat, acoustic folk-pop tune, like something from The Lumineers or a classic children's movie)\n",
            "\n",
            "**(Verse 1)**\n",
            "In an old oak tree, in the city park square\n",
            "Lived a squirrel with a twitch in his tail and his hair\n",
            "His name was Barnaby Scamp, and he wasn't content\n",
            "With the nuts of the present that providence sent\n",
            "For he'd found a strange object, buried under a root\n",
            "A gleaming gold walnut, a time-traveling fruit\n",
            "He called it the Chrononut, shiny and round\n",
            "One twist of the stem, and he'd leave the whole town!\n",
            "\n",
            "**(Chorus)**\n",
            "He's Barnaby Scamp, the temporal squirrel!\n",
            "Giving the timeline a flick and a twirl\n",
            "With a *WHIZZ* and a *POP* and a flash of his tail\n",
            "He's riding the currents on a historical gale\n",
            "From the past to the future, a nutty crusade\n",
            "For the greatest acorn that ever was made!\n",
            "\n",
            "**(Verse 2)**\n",
            "His first trip was backwards, a bit of a jump\n",
            "He landed in ferns with a sickening thump\n",
            "The air was all steamy, the trees were immense\n",
            "And a T-Rex's roar didn't make any sense!\n",
            "Barnaby chittered, his heart full of fright\n",
            "He dodged a big foot in the strange, muggy light\n",
            "But spied on a branch, a magnificent prize\n",
            "A ginkgo nut, prehistoric in size! He snatched it and vanished, a hero so bold!\n",
            "\n",
            "**(Chorus)**\n",
            "He's Barnaby Scamp, the temporal squirrel!\n",
            "Giving the timeline a flick and a twirl\n",
            "With a *WHIZZ* and a *POP* and a flash of his tail\n",
            "He's riding the currents on a historical gale\n",
            "From the past to the future, a nutty crusade\n",
            "For the greatest acorn that ever was made!\n",
            "\n",
            "**(Verse 3)**\n",
            "He zipped off to Rome, in its glorious prime\n",
            "Dodging the chariots, just in the nick of time\n",
            "He saw senators scheming in togas of white\n",
            "And scaled the Coliseum in the fading daylight\n",
            "A vendor was roasting some chestnuts below\n",
            "Barnaby plotted and put on a show\n",
            "He faked a loud sneeze from a statue's stone head\n",
            "And swiped a hot chestnut, then turned tail and fled!\n",
            "\n",
            "**(Bridge)**\n",
            "He’s seen the far future, the year Thirty-Ten\n",
            "Where the trees were all chrome, and the nuts grew in Zen\n",
            "He’s pilfered a pecan from a pirate's own sack\n",
            "And traded a walnut for getting it back\n",
            "He saw Isaac Newton, beneath an apple tree\n",
            "\"This one looks ripe!\" Barnaby thought with a glee\n",
            "He nudged it just so, with a nudge and a poke\n",
            "And that's how young Barnaby authored a joke on the whole of mankind!\n",
            "\n",
            "**(Guitar Solo - fast, playful, and full of frantic energy, like a squirrel scurrying up a tree)**\n",
            "\n",
            "**(Chorus)**\n",
            "He's Barnaby Scamp, the temporal squirrel!\n",
            "Giving the timeline a flick and a twirl\n",
            "With a *WHIZZ* and a *POP* and a flash of his tail\n",
            "He's riding the currents on a historical gale\n",
            "From the past to the future, a nutty crusade\n",
            "For the greatest acorn that ever was made!\n",
            "\n",
            "**(Outro)**\n",
            "Now back in his oak tree, the Chrononut sleeps\n",
            "Beside all the treasures his time-travel reaps\n",
            "A Roman roast chestnut, a ginkgo so grand\n",
            "The best collection of nuts in the land\n",
            "And Barnaby dreams, with a twitch of his nose\n",
            "Of the next nut to find, and the next place he goes...\n",
            "(Sound of a *whizz* and a *pop*, then silence)\n",
            "(Spoken, in a whisper)\n",
            "Where'd he go now?\n"
          ]
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "#### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UhNElguLRRNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae02ae4-0e2f-4d43-bed1-e8f5e374c903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "#### Compute tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d46645-7e28-438a-ece2-5313d0cebbf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") tokens_info=[TokensInfo(\n",
            "  role='user',\n",
            "  token_ids=[\n",
            "    1841,\n",
            "    235303,\n",
            "    235256,\n",
            "    573,\n",
            "    32514,\n",
            "    <... 6 more items ...>,\n",
            "  ],\n",
            "  tokens=[\n",
            "    b'What',\n",
            "    b\"'\",\n",
            "    b's',\n",
            "    b' the',\n",
            "    b' longest',\n",
            "    <... 6 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7065a8b6-1cb1-4f4c-dd43-7d58105d8d0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(\n",
              "  args={\n",
              "    'destination': 'Paris'\n",
              "  },\n",
              "  name='get_destination'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1Sn-VQE6_J"
      },
      "source": [
        "## Use context caching\n",
        "\n",
        "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\n",
        "\n",
        "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxTesUPIkNC"
      },
      "source": [
        "#### Create a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "adsuvFDA6xP5"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
        "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
        "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "pdf_parts = [\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    config=CreateCachedContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        contents=pdf_parts,\n",
        "        ttl=\"3600s\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBdQNHEoJmC5"
      },
      "source": [
        "#### Use a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "N8EhgCzlIoFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8239d0e7-ac90-4821-919d-71eb89be6e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The research paper introduces a family of multimodal models, Gemini, and this model improves the state-of-the-art in 30 of 32 benchmarks. A follow up paper, Gemini 1.5, reports on performance improvements on the multimodal models, improvements in text to image generation.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=\"What is the research goal shared by these research papers?\",\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhqrdiCer19"
      },
      "source": [
        "#### Delete a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rAUYcfOUdeoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19de39d-fc65-41b5-cc1e-e18a511d6ce8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeleteCachedContentResponse(\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=9>\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "client.caches.delete(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43be33d2672b"
      },
      "source": [
        "## Batch prediction\n",
        "\n",
        "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
        "\n",
        "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf948ae326b"
      },
      "source": [
        "### Prepare batch inputs\n",
        "\n",
        "The input for batch requests specifies the items to send to your model for prediction.\n",
        "\n",
        "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
        "\n",
        "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
        "\n",
        "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
        "- Located in `us-central1`\n",
        "- Appropriate read permissions for the service account\n",
        "\n",
        "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
        "\n",
        "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
        "\n",
        "```json\n",
        "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "81b25154a51a"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2031bb3f44c2"
      },
      "source": [
        "### Prepare batch output location\n",
        "\n",
        "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
        "\n",
        "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
        "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
        "\n",
        "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
        "\n",
        "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
        "\n",
        "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
        "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "fddd98cd84cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64df7295-71a3-4934-c91b-7de2506b88af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://artisan-task-manager-20251030222941/...\n",
            "BadRequestException: 400 Project 177778767202 may not create storageClass STANDARD buckets with locationConstraint GLOBAL.\n"
          ]
        }
      ],
      "source": [
        "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
        "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
        "\n",
        "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7da62c98880"
      },
      "source": [
        "### Send a batch prediction request\n",
        "\n",
        "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
        "\n",
        "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7ed3c2925663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4bfe52cf-7851-46b0-c59c-fbfa7223e0ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'projects/177778767202/locations/global/batchPredictionJobs/3476180778004512768'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "batch_job = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=INPUT_DATA,\n",
        "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
        ")\n",
        "batch_job.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bd49ff2c9e"
      },
      "source": [
        "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ee2ec586e4f1"
      },
      "outputs": [],
      "source": [
        "batch_job = client.batches.get(name=batch_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64eaf082ecb0"
      },
      "source": [
        "Optionally, you can list all the batch prediction jobs in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "da8e9d43a89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b7f998-b4a7-495f-804f-ade80e549902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projects/177778767202/locations/global/batchPredictionJobs/3476180778004512768 2025-10-30 22:30:00.865156+00:00 JobState.JOB_STATE_PENDING\n"
          ]
        }
      ],
      "source": [
        "for job in client.batches.list():\n",
        "    print(job.name, job.create_time, job.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de178468ba15"
      },
      "source": [
        "### Wait for the batch prediction job to complete\n",
        "\n",
        "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "c2187c091738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75099168-96a8-40d7-8143-db9b6b50a274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job failed: None\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Refresh the job until complete\n",
        "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
        "    time.sleep(5)\n",
        "    batch_job = client.batches.get(name=batch_job.name)\n",
        "\n",
        "# Check if the job succeeds\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    print(\"Job succeeded!\")\n",
        "else:\n",
        "    print(f\"Job failed: {batch_job.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0156eaf66675"
      },
      "source": [
        "### Retrieve batch prediction results\n",
        "\n",
        "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```json\n",
        "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "c2ce0968112c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a34f5d9a-cb66-4e55-8f50-8541e3ade98b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "b/artisan-task-manager-20251030222941/o",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4216081783.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gcs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"JOB_STATE_SUCCEEDED\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFSTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcoro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         allpaths = await self._find(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, path, withdirs, detail, prefix, versions, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;31m# Fetch objects as if the path is a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         objects, _ = await self._do_list_objects(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_do_list_objects\u001b[0;34m(self, path, max_results, delimiter, prefix, versions, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# sequential listing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             return await self._sequential_list_objects_helper(\n\u001b[0m\u001b[1;32m    680\u001b[0m                 \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_sequential_list_objects_helper\u001b[0;34m(self, bucket, delimiter, start_offset, end_offset, prefix, versions, page_size)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m         page = await self._call(\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;34m\"b/{}/o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     ):\n\u001b[1;32m    476\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{method.upper()}: {path}, {args}, {kwargs.get('headers')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         status, headers, info, contents = await self._request(\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         )\n",
            "\u001b[0;32m<decorator-gen-120>\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/retry.py\u001b[0m in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         except (\n\u001b[1;32m    137\u001b[0m             \u001b[0mHttpError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mvalidate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/retry.py\u001b[0m in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: b/artisan-task-manager-20251030222941/o"
          ]
        }
      ],
      "source": [
        "import fsspec\n",
        "import pandas as pd\n",
        "\n",
        "fs = fsspec.filesystem(\"gcs\")\n",
        "\n",
        "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
        "\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    # Load the JSONL file into a DataFrame\n",
        "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
        "\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81ccNPjiVzH"
      },
      "source": [
        "## Get text embeddings\n",
        "\n",
        "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zGOCzT7y31rk"
      },
      "outputs": [],
      "source": [
        "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "s94DkG5JewHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fa1136-61f3-425f-ca94-cccff06f8070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ContentEmbedding(\n",
            "  statistics=ContentEmbeddingStatistics(\n",
            "    token_count=18.0,\n",
            "    truncated=False\n",
            "  ),\n",
            "  values=[\n",
            "    -0.07045752555131912,\n",
            "    0.032266948372125626,\n",
            "    0.016506649553775787,\n",
            "    -0.03975583240389824,\n",
            "    -0.03190124034881592,\n",
            "    <... 123 more items ...>,\n",
            "  ]\n",
            "), ContentEmbedding(\n",
            "  statistics=ContentEmbeddingStatistics(\n",
            "    token_count=10.0,\n",
            "    truncated=False\n",
            "  ),\n",
            "  values=[\n",
            "    -0.040600020438432693,\n",
            "    0.01235074084252119,\n",
            "    -0.019680218771100044,\n",
            "    -0.01208257395774126,\n",
            "    -0.023434359580278397,\n",
            "    <... 123 more items ...>,\n",
            "  ]\n",
            "), ContentEmbedding(\n",
            "  statistics=ContentEmbeddingStatistics(\n",
            "    token_count=13.0,\n",
            "    truncated=False\n",
            "  ),\n",
            "  values=[\n",
            "    -0.08152230829000473,\n",
            "    0.013152849860489368,\n",
            "    -0.03257665038108826,\n",
            "    0.03197991102933884,\n",
            "    -0.04253409057855606,\n",
            "    <... 123 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.embed_content(\n",
        "    model=TEXT_EMBEDDING_MODEL_ID,\n",
        "    contents=[\n",
        "        \"How do I get a driver's license/learner's permit?\",\n",
        "        \"How do I renew my driver's license?\",\n",
        "        \"How do I change my address on my driver's license?\",\n",
        "    ],\n",
        "    config=EmbedContentConfig(output_dimensionality=128),\n",
        ")\n",
        "\n",
        "print(response.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "# What's next\n",
        "\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_genai_sdk.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}